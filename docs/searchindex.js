Search.setIndex({docnames:["2022-06/paper-20220629","2022-07/20220720_NeRF_original","2022-08/20220828_BEIT","2022-09/20220903_MAE","index","paper-template"],envversion:{"sphinx.domains.c":2,"sphinx.domains.changeset":1,"sphinx.domains.citation":1,"sphinx.domains.cpp":4,"sphinx.domains.index":1,"sphinx.domains.javascript":2,"sphinx.domains.math":2,"sphinx.domains.python":3,"sphinx.domains.rst":2,"sphinx.domains.std":2,sphinx:56},filenames:["2022-06\\paper-20220629.md","2022-07\\20220720_NeRF_original.md","2022-08\\20220828_BEIT.md","2022-09\\20220903_MAE.md","index.rst","paper-template.md"],objects:{},objnames:{},objtypes:{},terms:{"(dosovitskiy":2,"-consuming":0,"-image":2,"-oct":0,"-scale":0,"-softmax":2,"-supervised":3,"-to":2,"-training":4,"-zadeh":0,".,":2,"\u00e1r":3,"\u2605\u2605":[2,3],"\u2605\u2606":[0,1,5],"\u2606\u2606":[0,1,5],"\u3042\u308b":0,"\u3044\u308b":[0,2],"\u304b\u3051":2,"\u3050\u3089\u3044":[0,2],"\u3053\u3068":2,"\u3053\u306e":2,"\u3054\u3068":2,"\u3057\u304b":0,"\u3059\u308b":2,"\u305b\u308b":2,"\u305d\u306e":2,"\u3067\u304d":0,"\u3068\u3044\u3046":2,"\u306a\u3044":[0,2],"\u306a\u304f":2,"\u306a\u3089":2,"\u306b\u3088\u3063":2,"\u306b\u3088\u308b":0,"\u306b\u5bfe\u3057":2,"\u306e\u3067":[0,2],"\u307e\u307e":2,"\u307e\u307e\u3067":2,"\u3082\u306e":2,"\u3088\u3046":2,"\u3089\u308c":2,"\u308c\u308b":2,"\u30b5\u30a4\u30ba":0,"\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc":2,"\u30c8\u30fc\u30af\u30f3":2,"\u30d1\u30c3\u30c1":2,"\u30d4\u30af\u30bb\u30eb":2,"\u30da\u30fc\u30b8":4,"\u30de\u30b9\u30af":2,"\u30e2\u30b8\u30e5\u30fc\u30eb":4,"\u30ec\u30d9\u30eb":0,"\u4f5c\u6210":2,"\u4f7f\u3063":2,"\u4f7f\u308f":2,"\u4f7f\u7528":2,"\u4f9d\u5b58":2,"\u51dd\u96c6":0,"\u5229\u7528":2,"\u534a\u5206":0,"\u539a\u3055":0,"\u53ef\u80fd":2,"\u540c\u69d8":2,"\u5468\u6ce2":2,"\u554f\u984c":0,"\u5909\u5316":0,"\u5909\u63db":2,"\u5b66\u7fd2":2,"\u5bfe\u5fdc":2,"\u5f62\u614b":0,"\u5fae\u5206":2,"\u624b\u6cd5":2,"\u63a8\u5b9a":2,"\u63d0\u6848":0,"\u65b9\u6cd5":[0,2],"\u672c\u5f53":0,"\u672c\u8ad6":2,"\u691c\u7d22":4,"\u691c\u8a3c":2,"\u6a19\u6e96":2,"\u6a5f\u69cb":2,"\u6e2c\u308b":0,"\u7528\u3044":2,"\u753b\u50cf":2,"\u7570\u306a\u308a":2,"\u76f4\u63a5":2,"\u7d22\u5f15":4,"\u7d30\u80de":0,"\u7d50\u679c":2,"\u7dd1\u5185":0,"\u7dd1\u5185\u969c":0,"\u7fd2\u5f97":2,"\u826f\u3044":2,"\u8352\u3044":0,"\u89b3\u5bdf":0,"\u8a08\u6e2c":0,"\u8a3a\u65ad":0,"\u8a66\u3057":2,"\u8d77\u304d":0,"\u8ddd\u96e2":2,"\u8fd1\u3044":2,"\u9069\u7528":2,"\u90e8\u5206":2,"\u95a2\u4fc2":2,"\u9686\u8d77":0,"\u969c\u60a3":0,"\u969c\u8a3a":0,"\u975e\u9023":2,"\u9ad8\u3044":2,"for":4,"this":3,adaptive:4,affiliation:[0,1,2,3,5],after:3,ai:3,al:2,and:[0,3],any:3,ao:0,are:4,as:4,assessment:4,author:[0,1,2,3,5],autoencoders:4,backbone:2,bao:2,beit:[3,4],bert:4,better:3,but:3,cell:4,chen:3,classifcation:2,clinical:0,damage:4,date:[0,1,2,3,5],did:3,differences:3,different:3,does:3,doll:3,dong:2,duck:0,duke:0,encoder:3,et:2,evalation:3,experiments:3,facebook:3,farsiu:0,fields:4,from:4,furu:2,ganglion:4,gc:0,gcc:0,girshick:3,glaucomatous:4,gumble:2,hangbo:2,he:3,image:4,imagenet:3,images:4,individual:4,input:3,just:3,kaiming:3,large:0,leads:3,learnears:4,learning:3,li:[2,3],link:3,mae:3,main:3,maked:4,masked:3,microsoft:2,nerf:4,network:2,neural:4,not:[0,3],oct:4,of:4,on:3,optics:4,paper:3,parts:3,patches:3,performances:3,piotr:3,practical:0,pre:4,prediction:[1,5],published:3,quite:3,radiance:4,ramesh:2,rating:[0,1,2,3,5],reading:[0,1,2,3,5],relaxation:2,representing:4,research:[2,3],ross:3,saining:3,scalable:4,scenes:4,segmentation:[2,4],self:3,semantic:2,shot:2,similar:3,sina:0,soltanian:0,somayyeh:0,some:3,studies:0,subjective:0,supervised:4,synthesis:4,tag:[0,1,2,3,5],text:2,the:3,they:3,things:3,time:0,to:3,token:2,tokenize:2,tokenizer:3,tokennizer:2,tokens:2,transfer:3,transformer:[1,2,3,5],transformers:4,two:3,univ:0,use:[0,3],vae:2,vaswani:2,video:[1,5],view:4,vision:4,visual:2,vit:2,was:3,weakly:4,wei:2,which:3,xie:3,xinlei:3,yanghao:3,zero:2},titles:["Weakly supervised individual ganglion cell segmentation from adaptive optics OCT images for glaucomatous damage assessment","NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","BEIT: BERT Pre-Training of Image Transformers","Maked Autoencoders Are Scalable Vision Learnears","Welcome to paper-notes's documentation!","Title"],titleterms:{"'s":4,"-notes":4,"-training":2,"\u3069\u3093\u306a":0,"\u30e1\u30e2":[0,1,2,5],"\u5148\u884c":[0,1,2,5],"\u60c5\u5831":[0,1,2,3,5],"\u66f8\u304d":[0,1,2,5],"\u6bd4\u8f03":[0,1,2,5],"\u7814\u7a76":[0,1,2,5],"\u8ad6\u6587":[0,1,2,3,5],"for":[0,1],aaa:[1,2,5],adaptive:0,and:4,any:[0,1,2,5],are:3,as:1,assessment:0,autoencoders:3,beit:2,bert:2,cell:0,damage:0,discussion:[0,1,2,5],documentation:4,ee:[1,2,5],evaluation:[0,1,2,5],fields:1,from:0,ganglion:0,glaucomatous:0,ideas:[0,1,2,5],image:2,images:0,indices:4,individual:0,introduction:[0,1,2,5],key:[0,1,2,5],learnears:3,maked:3,method:[0,1,2,5],nerf:1,neural:1,next:[0,1,2,5],oct:0,of:2,optics:0,overview:[0,1,2,3,5],paper:4,papers:[0,1,2,5],pre:2,radiance:1,read:[0,1,2,5],representing:1,rere:[1,2,5],scalable:3,scenes:1,segmentation:0,supervised:0,synthesis:1,tables:4,template:[],title:5,to:[0,1,2,4,5],transformers:2,view:1,vision:3,weakly:0,welcome:4}})